{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification With Word Embeddings - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lesson, we'll use everything we've learned in this section to perform text classification using word embeddings!\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "Load the data, and all the frameworks and libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\medio\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load our dataset. We'll be working with the same dataset we worked with in the previous lab for this section, which you'll find inside `News_Category_Dataset_v2.zip`.  **_Go into the repo and unzip this file before continuing._**\n",
    "\n",
    "Once you've unzipped this dataset, go ahead and use pandas to read the data stored in `News_Category_Dataset_v2.json` in the cell below. Then, display the head of the DataFrame to ensure everything worked correctly. \n",
    "\n",
    "**_NOTE:_** When using the `pd.read_json()` function, be sure to include the `lines=True` parameter, or else it will crash!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40171\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>authors</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>headline</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20315</th>\n",
       "      <td>Janis Ian, ContributorSongwriter, author, perf...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>2017-07-28</td>\n",
       "      <td>This Is Not Government. It's Savagery.</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/this-is-n...</td>\n",
       "      <td>I want to move to Hawaii solely to be able to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105941</th>\n",
       "      <td>Amanda Terkel</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>2014-11-23</td>\n",
       "      <td>GOP Senator Urges Republicans To Move On From ...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jeff-flak...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173890</th>\n",
       "      <td>Marcus Samuelsson, Contributor\\nAward-Winning ...</td>\n",
       "      <td>FOOD &amp; DRINK</td>\n",
       "      <td>2012-11-14</td>\n",
       "      <td>10 Recipes, 10 Ways To Deliciously Use Your Th...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/10-recipe...</td>\n",
       "      <td>My 10 favorite ways to make sure you're not wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177657</th>\n",
       "      <td>Michelle Manetti</td>\n",
       "      <td>HOME &amp; LIVING</td>\n",
       "      <td>2012-10-04</td>\n",
       "      <td>Doorless Refrigerator Wall By Electrolux Desig...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/doorless-...</td>\n",
       "      <td>Hey, here's a way to subtract a step between y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97563</th>\n",
       "      <td>Jon Hartley, ContributorWorld Economic Forum G...</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>2015-02-27</td>\n",
       "      <td>The Emerging Markets Housing Bubble</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/the-emerg...</td>\n",
       "      <td>While in most advanced economies, housing pric...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  authors       category  \\\n",
       "20315   Janis Ian, ContributorSongwriter, author, perf...       POLITICS   \n",
       "105941                                      Amanda Terkel       POLITICS   \n",
       "173890  Marcus Samuelsson, Contributor\\nAward-Winning ...   FOOD & DRINK   \n",
       "177657                                   Michelle Manetti  HOME & LIVING   \n",
       "97563   Jon Hartley, ContributorWorld Economic Forum G...       BUSINESS   \n",
       "\n",
       "             date                                           headline  \\\n",
       "20315  2017-07-28             This Is Not Government. It's Savagery.   \n",
       "105941 2014-11-23  GOP Senator Urges Republicans To Move On From ...   \n",
       "173890 2012-11-14  10 Recipes, 10 Ways To Deliciously Use Your Th...   \n",
       "177657 2012-10-04  Doorless Refrigerator Wall By Electrolux Desig...   \n",
       "97563  2015-02-27                The Emerging Markets Housing Bubble   \n",
       "\n",
       "                                                     link  \\\n",
       "20315   https://www.huffingtonpost.com/entry/this-is-n...   \n",
       "105941  https://www.huffingtonpost.com/entry/jeff-flak...   \n",
       "173890  https://www.huffingtonpost.com/entry/10-recipe...   \n",
       "177657  https://www.huffingtonpost.com/entry/doorless-...   \n",
       "97563   https://www.huffingtonpost.com/entry/the-emerg...   \n",
       "\n",
       "                                        short_description  \n",
       "20315   I want to move to Hawaii solely to be able to ...  \n",
       "105941                                                     \n",
       "173890  My 10 favorite ways to make sure you're not wa...  \n",
       "177657  Hey, here's a way to subtract a step between y...  \n",
       "97563   While in most advanced economies, housing pric...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('News_Category_Dataset_v2.json', lines=True)\n",
    "df = df.sample(frac=0.2)\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's tranform our dataset as we did in the previous lab. \n",
    "\n",
    "In the cell below:\n",
    "\n",
    "*  Store the column that will be our target, `category`, in the variable `target`.\n",
    "* Combine the `headline` and `short_description` columns and store the result in a column called `combined_text`. When concatenating these two columns, make sure they are separated by a space character (`' '`)!\n",
    "* Use the `combined_text` column's map function to use the `word_tokenize` function on every piece of text. \n",
    "* Store the `.values` from the newly tokenized `combined_text` column inside the variable data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df.category\n",
    "df['combined_text'] = df.headline + ' ' + df.short_description\n",
    "data = df['combined_text'].map(word_tokenize).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading A Pretrained GloVe Model\n",
    "\n",
    "For this lab, we'll be loading the pretrained weights from **_GloVe_** (short for _Global Vectors for Word Representation_) from the [Stanford NLP Group](https://nlp.stanford.edu/projects/glove/).  These are commonly accepted as some of the best pre-trained word vectors available, and they're open source, so we can get them for free! Even the smallest file is still over 800 MB, so you'll you need to download this file manually. \n",
    "\n",
    "Note that there are several different sizes of pretrained word vectors available for download from the page linked above--for our purposes, we'll only need to use the smallest one, which still contains pretrained word vectors for over 6 billion words and phrases! To download this file, follow the link above and select the file called `glove.6b.zip`.  For simplicity's sake, you can also start the download by clicking [this link](http://nlp.stanford.edu/data/glove.6B.zip).  We'll be using the GloVe file containing 100-dimensional word vectors for 6 billion words. Once you've downloaded the file, unzip it, and move the file `glove.6B.50d.txt` into the same directory as this jupyter notebook. \n",
    "\n",
    "#### Getting the Total Vocabulary\n",
    "\n",
    "Although our pretrained GloVe data contains vectors for 6 billion words and phrases, we don't need all of them. Instead, we only need the vectors for the words that appear in our dataset. If a word or phrase doesn't appear in our dataset, then there's no reason to waste memory storing the vector for that word or phrase. \n",
    "\n",
    "This means that we need to start by computing the total vocabulary of our dataset. We can do this by adding every word in the dataset into a python `set` object. This is easy, since we've already tokenized each comment stored within `data`.\n",
    "\n",
    "In the cell below, add every token from every comment in data into a set, and store the set in the variable `total_vocabulary`.\n",
    "\n",
    "**_HINT_**: Even though this takes a loop within a loop, you can still do this with a one-line list comprehension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocabulary = set(word for headline in data for word in headline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 71277 unique tokens in our dataset.\n"
     ]
    }
   ],
   "source": [
    "len(total_vocabulary)\n",
    "print(\"There are {} unique tokens in our dataset.\".format(len(total_vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have gotten our total vocabulary, we can get the appropriate vectors out of the GloVe file. \n",
    "\n",
    "For the sake of expediency, we've included the code to read the appropriate vectors from the file. The code includes comments to explain what it is doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {}\n",
    "with open('glove.6B.50d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocabulary:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the cell above, we now have all of the words and their corresponding vocabulary stored within our dictionary, `glove`, as key/value pairs. \n",
    "\n",
    "Let's double-check that everything worked by getting the vector for a word from our `glove` dictionary. It's probably safe to assume that the word 'school' will be mentioned in at least one news headline, so let's get the vector for it. \n",
    "\n",
    "Get the vector for the word `'school'` from `glove` in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.90629  ,  1.2485   , -0.79692  , -1.4027   , -0.038458 ,\n",
       "       -0.25177  , -1.2838   , -0.58413  , -0.11179  , -0.56908  ,\n",
       "       -0.34842  , -0.39626  , -0.0090178, -1.0691   , -0.35368  ,\n",
       "       -0.052826 , -0.37056  ,  1.0931   , -0.19205  ,  0.44648  ,\n",
       "        0.45169  ,  0.72104  , -0.61103  ,  0.6315   , -0.49044  ,\n",
       "       -1.7517   ,  0.055979 , -0.52281  , -1.0248   , -0.89142  ,\n",
       "        3.0695   ,  0.14483  , -0.13938  , -1.3907   ,  1.2123   ,\n",
       "        0.40173  ,  0.4171   ,  0.27364  ,  0.98673  ,  0.027599 ,\n",
       "       -0.8724   , -0.51648  , -0.30662  ,  0.37784  ,  0.016734 ,\n",
       "        0.23813  ,  0.49411  , -0.56643  , -0.18744  ,  0.62809  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['school']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great--it worked!  Now that we've gotten the word vectors for every word in the  dataset, our next step is to combine all the vectors for a given headline into a **_Mean Embedding_** by finding the average of all the vectors in that headline. \n",
    "\n",
    "### Creating Mean Word Embeddings\n",
    "\n",
    "For this step, it's worth the extra effort to write our own mean embedding vectorizer class, so that we can make use of pipelines from scikit-learn. Using pipelines will save us time and make our code a bit cleaner. \n",
    "\n",
    "We've provided the code the mean embedding vectorizer class, with comments explaining what each step is doing. Take a minute to examine it and try to understand what the code is doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "    \n",
    "    # Note from Mike: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # It can't be used in a sklearn Pipeline. \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pipelines\n",
    "\n",
    "Since we've created a mean vectorizer class, we can pass this in as the first step in our pipeline, and then follow it up with the model we'll feed the data into for classification. \n",
    "\n",
    "Run the cell below to create pipeline objects that make use of the mean embedding vectorizer that we built above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf =  Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "              (\"Random Forest\", RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "                ('Support Vector Machine', SVC())])\n",
    "lr = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "              ('Logistic Regression', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create a list that contains a tuple for each pipeline, where the first item in the tuple is a name, and the second item in the list is the actual pipeline object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [('Random Forest', rf),\n",
    "          (\"Support Vector Machine\", svc),\n",
    "          (\"Logistic Regression\", lr)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the list we've created above, as well as the `cross_val_score` function from scikit-learn to train all the models, and store their cross validation score in an array. \n",
    "\n",
    "**_NOTE:_** Running the cell below may take a few minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   19.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   21.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.4s finished\n",
      "C:\\Users\\medio\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\medio\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "C:\\Users\\medio\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\medio\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\medio\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\medio\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "scores = [(name, cross_val_score(model, data, target, cv=2).mean()) for name, model, in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Random Forest', 0.3195585321385761),\n",
       " ('Support Vector Machine', 0.3025076172472023),\n",
       " ('Logistic Regression', 0.3280980534586512)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores may seem pretty low, but remember that there are 41 possible categories that headlines could be classified into. This means the naive accuracy rate (random guessing) would achieve an accuracy of just over 0.02! Our models have plenty of room for improvement, but they do work!\n",
    "\n",
    "### Deep Learning With Word Embeddings\n",
    "\n",
    "To end this lab, we'll see an example of how we can use an **_Embedding Layer_** inside of a Deep Neural Network to compute our own word embedding vectors on the fly, right inside our model! \n",
    "\n",
    "Don't worry if you don't understand the code below just yet--we'll be learning all about **_Sequence Models_** like the one below in our next section!\n",
    "\n",
    "Run the cells below.\n",
    "\n",
    "First, we'll import everything we'll need from Keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll convert our labels to a one-hot encoded format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(target).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll preprocess our text data. To do this, we start from the step where we combined the headlines and short description. We'll then use Keras's preprocessing tools to tokenize each example, convert them to sequences, and then pad the sequences so they're all the same length. \n",
    "\n",
    "Note how during the tokenization step, we set a parameter to tell the tokenizer to limit our overall vocabulary size to the `20000` most important words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(list(df.combined_text))\n",
    "list_tokenized_headlines = tokenizer.texts_to_sequences(df.combined_text)\n",
    "X_t = sequence.pad_sequences(list_tokenized_headlines, maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll construct our neural network. Notice how the **_Embedding Layer_** comes second, after the input layer. In the Embedding Layer, we specify the size we want our word vectors to be, as well as the size of the embedding space itself.  The embedding size we specified is 128, and the size of the embedding space is best as the size of the total vocabulary that we're using. Since we limited the vocab to 20000, that's the size we choose for the embedding layer. \n",
    "\n",
    "Once our data has passed through an embedding layer, we feed this data into an LSTM layer, followed by a Dense layer, followed by output layer. We also add some Dropout layers after each of these layers, to help fight overfitting.\n",
    "\n",
    "Our output layer is a Dense layer with 41 neurons, which corresponds to the 41 possible classes in our labels. We set the activation function for this output layer to `'softmax'`, so that our network will output a vector of predictions, where each element's value corresponds to the percentage chance that the example is the class that corresponds to that element, and where the sum of all elements in the output vector is 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "input_ = Input(shape=(100,))\n",
    "x = Embedding(20000, embedding_size)(input_)\n",
    "x = LSTM(25, return_sequences=True)(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# There are 41 different possible classes, so we use 41 neurons in our output layer\n",
    "x = Dense(41, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have designed our model, we still have to compile it, and provide important parameters such as the loss function to use (`'categorical_crossentropy'`, since this is a mutliclass classification problem), and the optimizer to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After compiling the model, we quickly check the summary of the model to see what our model looks like, and make sure the output shapes line up with what we expect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 100, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100, 25)           15400     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                1300      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 41)                2091      \n",
      "=================================================================\n",
      "Total params: 2,578,791\n",
      "Trainable params: 2,578,791\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can fit the model by passing in the data, our labels, and setting some other hyperparameters such as the batch size, the number of epochs to train for, and what percentage of the training data to use for validation data. \n",
    "\n",
    "If trained for 3 epochs, we'll find the model achieves a validation accuracy of almost 41%. \n",
    "\n",
    "Run the cell below for 1 epoch. Note that this is a large network, so the training will take some time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36153 samples, validate on 4018 samples\n",
      "Epoch 1/2\n",
      "36153/36153 [==============================] - 184s 5ms/step - loss: 2.6305 - acc: 0.3169 - val_loss: 2.4481 - val_acc: 0.3616\n",
      "Epoch 2/2\n",
      "36153/36153 [==============================] - 184s 5ms/step - loss: 2.3492 - acc: 0.3757 - val_loss: 2.3228 - val_acc: 0.4089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d757632f98>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_t, y, epochs=2, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 1 epoch, our model does about as well as the shallow algorithms we tried above. However, our LSTM Network was able to achieve a validation accuracy of over 40% after only 3 epochs of training. It's likely that if we trained for more epochs or added in the rest of the data, our performance would improve even further (but our run time would get much, much longer). \n",
    "\n",
    "It's common to embedding layers in LSTM networks, because both are special tools most commonly used for text data. The embedding layer creates it's own vectors based on the language in the text data it trains on, and then passes that information on to the LSTM network one word at a time. We'll learn more about LSTMs and other kinds of **_Recurrent Neural Networks_** in our next section!\n",
    "\n",
    "# Summary\n",
    "\n",
    "In this lab, we used everything we know about word embeddings to perform text classification, and then we built a Multi-Layer Perceptron model that incorporated a word embedding layer in it's own architecture!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
